{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./spam.csv\", encoding = \"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_train, \\\n",
    "data_test, \\\n",
    "label_train, \\\n",
    "label_test = train_test_split(data.v2, data.v1, test_size = 0.2, \n",
    "                              random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', ..., 'ham', 'ham', 'ham'], dtype=object)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114     ham\n",
       "3589     ham\n",
       "3095     ham\n",
       "1012     ham\n",
       "3320     ham\n",
       "4130     ham\n",
       "1197     ham\n",
       "5426     ham\n",
       "624      ham\n",
       "2260    spam\n",
       "Name: v1, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114    No no:)this is kallis home ground.amla home to...\n",
       "3589    I am in escape theatre now. . Going to watch K...\n",
       "3095    We walked from my moms. Right on stagwood pass...\n",
       "1012       I dunno they close oredi not... ÌÏ v ma fan...\n",
       "3320                               Yo im right by yo work\n",
       "4130    \\Its Ur luck to Love someone. Its Ur fortune t...\n",
       "1197     He also knows about lunch menu only da. . I know\n",
       "5426        Oh yeah! And my diet just flew out the window\n",
       "624     Nah it's straight, if you can just bring bud o...\n",
       "2260    SplashMobile: Choose from 1000s of gr8 tones e...\n",
       "Name: v2, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyNaiveBayes():\n",
    "    \n",
    "    def __int__(self,\n",
    "               p_each_word_spam = None,\n",
    "               p_each_word_ham = None,\n",
    "               p_spam = None,\n",
    "               p_ham = None,\n",
    "               word_set = None):\n",
    "         self.p_each_word_spam = p_each_word_spam\n",
    "         self.p_each_word_ham  = p_each_word_ham\n",
    "         self.p_spam           = p_spam\n",
    "         self.p_ham            = p_ham\n",
    "         word_set              = word_set\n",
    "            \n",
    "    # find all the unique words in the data(dataframw/matrix)\n",
    "    def getVocabularyFromDataframe(self, data) :\n",
    "        wordset = set([])\n",
    "\n",
    "        for document in data:\n",
    "            words = document.split()\n",
    "            for word in words:\n",
    "                wordset.add(word)\n",
    "        return list(wordset)\n",
    "    \n",
    "    # convert a string to a word vector\n",
    "    def documentToWordVector(self, word_set, doc):\n",
    "        vect = np.zeros(len(word_set))\n",
    "\n",
    "        words = doc.split()\n",
    "\n",
    "        for word in words:\n",
    "            if word in word_set:\n",
    "                vect[word_set.index(word)] += 1\n",
    "        return vect\n",
    "    \n",
    "    #convert all the rows in the data to word vectors\n",
    "    def getWordMatrix(self, data):\n",
    "        word_matrix = []\n",
    "        for document in data.values:\n",
    "            word_vector = documentToWordVector(word_set, document)\n",
    "            word_matrix.append(word_vector)\n",
    "        return word_matrix\n",
    "\n",
    "    # get the p(eachword|spam), p(eachword|ham), p(spam) and p(ham)\n",
    "    def fit(self, data_train, label_train):\n",
    "        \n",
    "        # get the entire vocabulary of the training data\n",
    "        word_set = getVocabularyFromDataframe(data_train)\n",
    "        train_matrix = self.getWordMatrix(data_train)\n",
    "\n",
    "        num_docs = len(train_matrix)\n",
    "        num_words = len(word_set)\n",
    "        \n",
    "        print(\"num_docs = \" + str(num_docs))\n",
    "        print(\"num_words = \" + str(num_docs))\n",
    "        \n",
    "        #laplace smoothing: numerator + 1, denominator + n\n",
    "        # number of appearance of each word in spam emails\n",
    "        vect_word_count_in_spams = np.ones(num_words)\n",
    "        vect_word_count_in_hams  = np.ones(num_words)\n",
    "        \n",
    "        # size of the vocabulary\n",
    "        spam_total_num_words = num_words\n",
    "        ham_total_num_words  = num_words\n",
    "\n",
    "        spam_emails_count = 0\n",
    "        ham_emails_count  = 0\n",
    "\n",
    "        # label_train's index is messed after split\n",
    "        # there maybe data missing \n",
    "        # so better not to use label_train[i]\n",
    "        # but use label_train.values\n",
    "        for i in range(num_docs):\n",
    "            if label_train.values[i] == \"spam\":\n",
    "                # python can add items by vector directly\n",
    "                vect_word_count_in_spams += train_matrix[i]\n",
    "                spam_total_num_words += sum(train_matrix[i])\n",
    "                spam_emails_count += 1\n",
    "            else:\n",
    "                vect_word_count_in_hams   += train_matrix[i]\n",
    "                ham_total_num_words += sum(train_matrix[i])\n",
    "                spam_emails_count   += 1\n",
    "\n",
    "        self.p_each_word_spam = np.log(vect_word_count_in_spams/spam_total_num_words)\n",
    "        self.p_each_word_ham = np.log(vect_word_count_in_hams/ham_total_num_words)\n",
    "\n",
    "        self.p_spam = np.log(spam_emails_count/num_docs)\n",
    "        self.p_ham = np.log(spam_emails_count/num_docs)\n",
    "\n",
    "    def predict(self, data_test):\n",
    "        test_vector_matrix = self.getWordMatrix(data_test)\n",
    "        result_spam = np.matmul(test_vector_matrix, self.p_each_word_spam.transpose()) + self.p_spam.transpose()\n",
    "        result_ham  = np.matmul(test_vector_matrix, self.p_each_word_ham.transpose())  + self.p_ham.transpose()\n",
    "        result = result_spam - result_ham\n",
    "        vect_func = np.vectorize(lambda x:\"spam\" if x > 0 else \"ham\")\n",
    "        result = vect_func(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_docs = 4457\n",
      "num_words = 4457\n"
     ]
    }
   ],
   "source": [
    "model = MyNaiveBayes()\n",
    "model.fit(data_train, label_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -8.83590144, -10.2221958 , -10.2221958 , ...,  -9.52904862,\n",
       "       -10.2221958 , -10.2221958 ])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.p_each_word_spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', ..., 'ham', 'ham', 'ham'], dtype='<U4')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([948.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 167.]),\n",
       " array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a0b203780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9668161434977578\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        ham       0.98      0.98      0.98       949\n",
      "       spam       0.89      0.89      0.89       166\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1115\n",
      "\n",
      "[[930  19]\n",
      " [ 18 148]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(accuracy_score(label_test, predictions))\n",
    "print(classification_report(label_test, predictions))\n",
    "print(confusion_matrix(label_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

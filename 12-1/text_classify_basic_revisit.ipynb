{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20000000 241088714 1191619649 segmented_train_seg_by_word.txt\r\n"
     ]
    }
   ],
   "source": [
    "#?????\n",
    "! wc segmented_train_seg_by_word.txt   # in this txt file the chinese lines are in the format of word + space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "from utils import ProgressBar # show the process of a long running operation\n",
    "\n",
    "import keras.preprocessing.sequence\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "enline = None\n",
    "chline = None\n",
    "\n",
    "sentlength = 4  # sentence less than 4 words\n",
    "\n",
    "enlines = []\n",
    "chlines = []\n",
    "\n",
    "#pb = progressbar.ProgressBar(worksum = 100000000)\n",
    "#pb.startjob()\n",
    "num = 0\n",
    "with open(\"segmented_train_seg_by_word.txt\") as fhdl:\n",
    "    for line in fhdl:\n",
    "        num += 1\n",
    "        if num % 2 == 1:\n",
    "            enline = line\n",
    "            continue\n",
    "        else:\n",
    "            chline = line\n",
    "        \n",
    "        # split english line and cliense line to words list\n",
    "        enlinesp = [word.lower() for word in enline.strip(\"\\n\").split()]\n",
    "        chlinesp = [word for word in chline.strip(\"\\n\").split()]\n",
    "        \n",
    "        # only use sentences <= 4 words to reduce running time\n",
    "        if  len(enlinesp) <= sentlength and len(chlinesp) <= sentlength:\n",
    "            enlines.append(enlinesp)\n",
    "            chlines.append(chlinesp)\n",
    "            \n",
    "        # log the progress when reachine 2000\n",
    "        if (num//2) % 10000 == 0:\n",
    "            #pb.complete(1000)\n",
    "            #print(num)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150904, 150904)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enlines), len(chlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deuces', 'the', 'winner', '.'],\n",
       " ['one', 'against', '500', '.'],\n",
       " ['a', 'twin', '.'],\n",
       " ['twin', 'girls', '.'],\n",
       " ['identical', 'twins', '.'],\n",
       " ['pair', 'of', 'underachievers', '.'],\n",
       " ['husband', 'and', 'wife', '.'],\n",
       " ['couple', '.'],\n",
       " ['nice', 'couple', '.'],\n",
       " ['a', 'young', 'couple', '.']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enlines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['deuces', 'the', 'winner', '.'],\n",
       " ['one', 'against', '500', '.'],\n",
       " ['a', 'twin', '.'],\n",
       " ['twin', 'girls', '.'],\n",
       " ['identical', 'twins', '.'],\n",
       " ['pair', 'of', 'underachievers', '.'],\n",
       " ['husband', 'and', 'wife', '.'],\n",
       " ['couple', '.'],\n",
       " ['nice', 'couple', '.'],\n",
       " ['a', 'young', 'couple', '.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enlines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "# put all the chinese and english word lists to one word list\n",
    "# to statistic all the words\n",
    "for sent in chlines:\n",
    "    for word in sent:\n",
    "        words.append(word)\n",
    "\n",
    "for sent in enlines:\n",
    "    for word in sent:\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 69213),\n",
       " ('。', 64997),\n",
       " ('？', 18494),\n",
       " ('?', 18486),\n",
       " ('的', 15156),\n",
       " ('...', 15073),\n",
       " (',', 13814),\n",
       " ('，', 12598),\n",
       " ('the', 11865),\n",
       " ('我', 11374),\n",
       " ('！', 11165),\n",
       " ('!', 10976),\n",
       " ('i', 8885),\n",
       " ('你', 7707),\n",
       " ('了', 7633)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(words).most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#??????????????\n",
    "def addchar(what2ind, ind2what, c):\n",
    "    if c in what2ind:\n",
    "        return\n",
    "    ind2what[len(what2ind)] = c\n",
    "    what2ind[c] = len(what2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a dictionary\n",
    "# i  1\n",
    "# am 2\n",
    "# teacher <unk>\n",
    "\n",
    "word2ind = {}    # dictionary from word to index\n",
    "ind2word = {}    # dictionary from index to word\n",
    "\n",
    "specialchars = [\"<pad>\", \"<unk>\"]\n",
    "\n",
    "for one in specialchars:\n",
    "    addchar(word2ind, ind2word, one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word,_ in Counter(words).most_common(10000):   # add the 1000 most common words into the dictionary\n",
    "    addchar(word2ind,ind2word,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_x_in = []\n",
    "dat_y    = []\n",
    "\n",
    "for chsent, ensent in zip(chlines, enlines):\n",
    "    indsent = [word2ind.get(i, word2ind[\"<unk>\"]) for i in chsent]\n",
    "    dat_x_in.append(indsent)\n",
    "    dat_y.append(0)\n",
    "    \n",
    "    indsent = [word2ind.get(i, word2ind[\"<unk>\"]) for i in ensent]\n",
    "    dat_x_in.append(indsent)\n",
    "    dat_y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_x_in = keras.preprocessing.sequence.pad_sequences(dat_x_in,\n",
    "                                                        padding = \"post\",\n",
    "                                                        value = word2ind[\"<pad>\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_y = np.asarray(dat_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((301808, 4), (301808,))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_x_in.shape, dat_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    1,    3,    0],\n",
       "       [   1,   10, 5685,    2],\n",
       "       [2188,    1,    1,    3],\n",
       "       [  62, 1053, 4818,    2],\n",
       "       [2188, 3712,    3,    0],\n",
       "       [  17, 4273,    2,    0],\n",
       "       [2188, 3712,  589,    3],\n",
       "       [4273,  761,    2,    0],\n",
       "       [2188,    1, 3712,    3],\n",
       "       [8988, 3941,    2,    0]], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_x_in[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, numpy.ndarray)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dat_x_in), type(dat_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(batch_size):\n",
    "    # array = []\n",
    "    # delete while True\n",
    "    while True:  # why not infinite loop\n",
    "        for i in range(0, len(dat_x_in), batch_size):\n",
    "            if i + batch_size <len(dat_x_in):\n",
    "                yield dat_x_in[i:i + batch_size], dat_y[i:i + batch_size] \n",
    "                # array.append()\n",
    "                # create batches and hannging them there\n",
    "                #[n, batch_size, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = data_generator(256)   # batch size 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = gen.__next__()  # everytime run the generator next, run one iteration till the yield, then stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,    1,    3,    0],\n",
       "       [   1,   10, 5685,    2],\n",
       "       [2188,    1,    1,    3],\n",
       "       ...,\n",
       "       [3791,    1,    2,    0],\n",
       "       [2564,    1,    4,    0],\n",
       "       [  17, 4894,    5,    0]], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((256, 4), (256,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape, batch_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "embedding_size = 100\n",
    "vocabulary_size = len(ind2word)\n",
    "num_units = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f0d83b41d5e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtflearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tflearn'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto(log_device_placement=True, \n",
    "                        allow_soft_placement=True )\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith tf.device(\"/gpu:1\"): # ?????\\n    initializer = tf.random_uniform_initializer(-0.08, 0.08)\\n    tf.get_variable_scope().set_initializer(initializer)\\n    \\n    x = tf.placeholder(\"int32\", [None, None])\\n    y = tf.placeholder(\"int32\", None)\\n    x_len = tf.placeholder(\"int32\", [None])\\n    \\n    learning_rate = tf.placeholder(tf.float32, shape=[])\\n    \\n    embedding_encoder = tf.get_variable( \"embedding_encoder\",\\n                                       [vocabulary_size, embedding_size],\\n                                       dtype = tf.float32)\\n    encoder_emb_inp = tf.nn.embedding_lookup(embedding_encoder, x)\\n    \\n    encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\\n    \\n    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,\\n                                                    encoder_emb_inp,\\n                                                    sequence_length = x_len,\\n                                                    time_major=False,\\n                                                    dtype = tf.float32)\\n    model_logistic = tf.layers.dense(encoder_state[0], 1)\\n    model_pred = tf.nn.sigmoid(model_logistic)\\n    \\n    loss = tf.nn.sigmoid_cross_entropy_with_logits(\\n        labels = tf.cast(y,tf.float32),\\n        logits = tf.reshape(model_logistic, (-1,)))\\n    loss = tf.reduce_mean(loss)\\n    optimizer = tf.train.GradientDescentOptimizer(\\n        learning_rate = learning_rate).minimize(loss)\\n'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.device(\"/gpu:1\"): \n",
    "    initializer = tf.random_uniform_initializer(-0.08, 0.08)\n",
    "    tf.get_variable_scope().set_initializer(initializer)\n",
    "    \n",
    "    x = tf.placeholder(\"int32\", [None, None])   #[batch_size, 句子长度] 二者都可调\n",
    "    y = tf.placeholder(\"int32\", None)\n",
    "    x_len = tf.placeholder(\"int32\", [None])\n",
    "    \n",
    "    learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "    \n",
    "    # word embedding\n",
    "    embedding_encoder = tf.get_variable( \"embedding_encoder\",\n",
    "                                       [vocabulary_size, embedding_size],     #【10000， 100】\n",
    "                                       dtype = tf.float32)\n",
    "    encoder_emb_inp = tf.nn.embedding_lookup(embedding_encoder, x)\n",
    "    \n",
    "    encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)\n",
    "    \n",
    "    encoder_outputs, encoder_state = tf.nn.dynamic_rnn(encoder_cell,      # 可变长的rnn 可以支持增加rnn的个数\n",
    "                                                    encoder_emb_inp,\n",
    "                                                    sequence_length = x_len,\n",
    "                                                    time_major=False,\n",
    "                                                    dtype = tf.float32)\n",
    "    \n",
    "    # fully connection layer fully connect to one classifiler\n",
    "    model_logistic = tf.layers.dense(encoder_state[0], 1)\n",
    "    model_pred = tf.nn.sigmoid(model_logistic)\n",
    "    \n",
    "    loss = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels = tf.cast(y,tf.float32),\n",
    "        logits = tf.reshape(model_logistic, (-1,)))\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(\n",
    "        learning_rate = learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())  # Returns an Op that initializes global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-9cd8911d4b4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbatch_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeginning_lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         _, batch_loss = session.run([optimizer, loss], \n\u001b[0m\u001b[1;32m     12\u001b[0m                                     feed_dict={\n\u001b[1;32m     13\u001b[0m                                                 \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "beginning_lr = 0.1\n",
    "gen = data_generator(batch_size)\n",
    "\n",
    "# 只运行一个epoch\n",
    "for one_epoch in range(0, 1):\n",
    "    for one_batch in range(1, len(dat_x_in), batch_size):\n",
    "        batch_x, batch_y = gen.__next__()\n",
    "        batch_x_len = np.asarray([len(i) for i in batch_x])\n",
    "        batch_lr = beginning_lr\n",
    "        \n",
    "        _, batch_loss = session.run([optimizer, loss], \n",
    "                                    feed_dict={\n",
    "                                                x:batch_x,\n",
    "                                                y:batch_y,\n",
    "                                                x_len:batch_x_len,\n",
    "                                                learning_rate:batch_lr,\n",
    "                                               }\n",
    "                                   )\n",
    "        losses.append(batch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-57-65792a5c845e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-57-65792a5c845e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    matplotlib inline\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "matplotlib inline\n",
    "pd.Dataframe(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-58-566bed6efba9>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-58-566bed6efba9>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    return “英文” if batch_predict > 0.5 else \"中文\"\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "def predict_result(sent):\n",
    "    senttoken = [word2ind[i.lower()] for i in sent.split()]\n",
    "    inputx = np.asarray([senttoken])\n",
    "    inputx_len = np.asarray([len(senttoken)])\n",
    "    \n",
    "    print(inputx, inputx_len)\n",
    "    batch_predict = session.run(model_pred,\n",
    "                               feed_dict = {\n",
    "                                   x:inputx,\n",
    "                                   x_len:inputx_len,\n",
    "                               })[0]\n",
    "    return “英文” if batch_predict > 0.5 else \"中文\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentence = \"who are you, what do you want .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result(source_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
